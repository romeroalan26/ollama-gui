# Chat con IA Local - Ollama

Una aplicaci√≥n web simple para chatear con modelos de IA locales usando Ollama.

## üöÄ Caracter√≠sticas

- **Interfaz moderna y responsive**: Dise√±o limpio y f√°cil de usar
- **Conexi√≥n directa a Ollama**: Se conecta a tu servidor Ollama local
- **M√∫ltiples modelos**: Soporte para diferentes modelos de IA
- **Configuraci√≥n personalizable**: URL del API y modelo configurable
- **Historial de conversaciones**: Guarda y exporta tus conversaciones
- **Atajos de teclado**: Navegaci√≥n r√°pida con teclado
- **Indicador de escritura**: Muestra cuando la IA est√° procesando

## üìã Requisitos

- [Ollama](https://ollama.ai/) instalado y ejecut√°ndose
- Un modelo de IA descargado (ej: `llama3:8b`)
- Navegador web moderno

## üõ†Ô∏è Instalaci√≥n

1. **Descarga Ollama** (si no lo tienes):

   ```bash
   # Windows (usando winget)
   winget install Ollama.Ollama

   # O descarga desde https://ollama.ai/
   ```

2. **Descarga un modelo**:

   ```bash
   ollama pull llama3:8b
   ```

3. **Ejecuta Ollama**:

   ```bash
   ollama serve
   ```

4. **Abre la aplicaci√≥n**:

   - Simplemente abre `index.html` en tu navegador
   - O usa un servidor local:

     ```bash
     # Python 3
     python -m http.server 8000

     # Node.js
     npx serve .

     # PHP
     php -S localhost:8000
     ```

## üéØ Uso

1. **Abrir la aplicaci√≥n**: Navega a `http://localhost:8000` (o abre `index.html` directamente)

2. **Configurar conexi√≥n**:

   - **URL del API**: `http://localhost:11434` (por defecto)
   - **Modelo**: Selecciona el modelo que quieres usar

3. **Chatear**:
   - Escribe tu mensaje en el √°rea de texto
   - Presiona `Enter` o haz clic en el bot√≥n de enviar
   - Espera la respuesta de la IA

## ‚å®Ô∏è Atajos de Teclado

- **Enter**: Enviar mensaje
- **Shift + Enter**: Nueva l√≠nea
- **Ctrl/Cmd + L**: Limpiar chat
- **Ctrl/Cmd + E**: Exportar conversaci√≥n

## ‚öôÔ∏è Configuraci√≥n

### Modelos Disponibles

La aplicaci√≥n incluye varios modelos predefinidos:

- `llama3:8b` - Modelo r√°pido y eficiente
- `llama3:70b` - Modelo m√°s potente (requiere m√°s recursos)
- `mistral:7b` - Modelo especializado en c√≥digo
- `codellama:7b` - Modelo optimizado para programaci√≥n

### Agregar Nuevos Modelos

Para agregar m√°s modelos:

1. Descarga el modelo con Ollama:

   ```bash
   ollama pull nombre-del-modelo
   ```

2. Edita `index.html` y agrega la opci√≥n:
   ```html
   <option value="nombre-del-modelo">nombre-del-modelo</option>
   ```

## üîß Soluci√≥n de Problemas

### Error de Conexi√≥n

Si ves "Error: HTTP 404" o similar:

1. **Verifica que Ollama est√© ejecut√°ndose**:

   ```bash
   curl http://localhost:11434/api/tags
   ```

2. **Verifica que el modelo est√© descargado**:

   ```bash
   ollama list
   ```

3. **Reinicia Ollama**:
   ```bash
   ollama serve
   ```

### Error CORS (Cross-Origin Resource Sharing)

Si ves "Failed to fetch" o errores de CORS:

#### Soluci√≥n 1: Usar un servidor local (Recomendado)

```bash
# Python 3
python -m http.server 8000

# Node.js
npx serve . -p 8000

# PHP
php -S localhost:8000
```

Luego abre `http://localhost:8000` en tu navegador.

#### Soluci√≥n 2: Usar el proxy incluido

```bash
# Ejecutar el proxy
python proxy.py

# Cambiar la URL en la aplicaci√≥n a:
http://localhost:8080
```

#### Soluci√≥n 3: Abrir con navegador especial

```bash
# Chrome (Windows)
chrome.exe --disable-web-security --user-data-dir="C:/temp/chrome_dev"

# Firefox
firefox -P "dev" --disable-web-security
```

### Modelo No Encontrado

Si el modelo no est√° disponible:

1. **Descarga el modelo**:

   ```bash
   ollama pull nombre-del-modelo
   ```

2. **Verifica el nombre exacto**:
   ```bash
   ollama list
   ```

## üìÅ Estructura del Proyecto

```
chat-ia-local/
‚îú‚îÄ‚îÄ index.html          # P√°gina principal
‚îú‚îÄ‚îÄ styles.css          # Estilos CSS
‚îú‚îÄ‚îÄ script.js           # L√≥gica JavaScript
‚îî‚îÄ‚îÄ README.md           # Este archivo
```

## üîÑ API de Ollama

La aplicaci√≥n usa la API REST de Ollama:

```bash
curl --location 'http://localhost:11434/api/generate' \
--header 'Content-Type: application/json' \
--data '{
  "model": "llama3:8b",
  "prompt": "Tu mensaje aqu√≠",
  "stream": false
}'
```

## ü§ù Contribuir

¬°Las contribuciones son bienvenidas! Puedes:

1. Reportar bugs
2. Sugerir nuevas caracter√≠sticas
3. Mejorar el dise√±o
4. Agregar soporte para m√°s modelos

## üìÑ Licencia

Este proyecto es de c√≥digo abierto y est√° disponible bajo la licencia MIT.

## üôè Agradecimientos

- [Ollama](https://ollama.ai/) por proporcionar la infraestructura de IA local
- La comunidad de modelos de IA de c√≥digo abierto
